<html>
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Newfy</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
    </head>

<div align="center">

# Newfy 

Ferramenta de captura e armazenamento de not√≠cias estrat√©gicas e dados relevantes publicados em API oficiais associados ao tema. Projeto voltado ao treino de  CRUD Web, Requisitos n√£o funcionais (Linguagem OO, BD Relacional)

### [Reposit√≥rio do projeto NewFy ](https://github.com/c137santos/FATEC-API-3-SEMESTRE/tree/main)

</div>

## Descri√ß√£o do projeto

Ferramenta de captura e armazenamento de not√≠cias estrat√©gicas e dados relevantes publicados em API oficiais associados ao tema. O desafio foi criar mac√¢nismo para o mapeamento de portais de not√≠cias cadastradas pelo cliente, para a captura do conte√∫do veiculado, com um armazenamento desses, formando assim um banco de dados hist√≥rico das publica√ß√µes. 

Al√©m disso, tamb√©m foi necess√°rio a captura e armazenamento de dados estrat√©gios provindos de API oficiais para fins de compara√ß√µes futuras. Exemplo, captura de dados relacionados a agricultura, e dados da API de dados climaticos do INPE, por exemplo, uma aferi√ß√£o de se as not√≠cias veiculadas no dia X sobre poss√≠vel seca correspondia as previs√µes do INPE daquela data. 

A empresa parceira foi a [GSW Sofware](https://pitsjc.org.br/empresas/gsw/?__cf_chl_tk=Oadw5xOIX8N3p9N3zMLQaaPAovMKruknHwLsSBh0faQ-1744635957-1.0.1.1-yYGVjlGCNsyG42toG2oO76EmA8jZF8RUiam24O2XQ_M). 


## üöÄ Tecnologias Utilizadas

- <i class="fas fa-server"></i> **Backend**: Java com framework Spring Boot  
- <i class="fas fa-database"></i> **Banco de Dados**: PostgreSQL  
- <i class="fas fa-exchange-alt"></i> **ORM**: Hibernate e flyway 
- <i class="fas fa-code"></i> **Frontend**: JavaScript com framework Vue.js  
- <i class="fab fa-docker"></i> **Containeriza√ß√£o**: Docker e DevContainer  


## Equipe

Nossa equipe foi batizada de Cerberus, e nesse time assumi o cargo de Scrum Master, e colaborei com os seguites membros:

| Integrante                                                                                                                             | LinkedIn                                                                                                                                                                |
| -------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Clara Santos ![Static Badge](https://img.shields.io/badge/Scrum_master-pink) ![Static Badge](https://img.shields.io/badge/Dev-black)   | [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/c137santos/)                  |
| Mar√≠lia Borgo ![Static Badge](https://img.shields.io/badge/Product_owner-blue) ![Static Badge](https://img.shields.io/badge/Dev-black) | [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/mariliaborgo/)                |
| Matheus Marciano ![Static Badge](https://img.shields.io/badge/Dev-black)                                                               | [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/matheus-marciano-leite/)      |
| Guilherme Bezerra Junqueira ![Static Badge](https://img.shields.io/badge/Dev-black)                                                    | [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/guilherme-bezerra-a01035170/) |
| Pedro Henrique Lopes de Souza ![Static Badge](https://img.shields.io/badge/Dev-black)                                                  | [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pelopes7/)                    |
| Yan Costa Yamim ![Static Badge](https://img.shields.io/badge/Dev-black)                                                                | [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/yan-yamim-185220278/)         |
| Eduardo da Silva Lima ![Static Badge](https://img.shields.io/badge/Dev-black)                                                          | [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/edu-datamarketing/)           |

## Contribui√ß√µes Individuais

Atuei como dev pontualmente. Nesse projeto, foquei mais na organiza√ß√£o da equipe e na comunica√ß√£o entre os membros. Al√©m disso, tamb√©m fui respons√°vel por retirar os impedimento que surgiu no caminho, manter o burdown evoluindo e ser TechLead dos integrantes. Ou seja, direcionava para onde as solu√ß√µes tecnicas deveriam caminhar. 

Por isso, minha primeira tarefa foi desenhar, discutir e implementar o banco de dados. Que foi ponto inicial do desenho do nosso produto. 

Organizei tamb√©m as documenta√ß√µes do projeto, exemplo do Manual do usu√°rio, documenta√ß√£o dos campos do banco de dados, kanbam na ferramenta do github project, criando cards a partir das userStores, issues, organiza√ß√£o de PR e de fluxo de CR.

Foram 78 cards entregues de 17 userStores.

Al√©m disso, aprendi o fechamento de vers√£o por tags e publica√ß√£o do release.

O maior aprendizado foi relativo a constru√ß√£o do banco de dados em java spring-boot, usando hibernate e flyway. A configura√ß√£o de ambos parecia conflitar, por isso tive que entender qual era o escopo de cada um, e como eles se comunicavam.

No `application.properties` direcionei o hibernate para ser um validador das entidades, retirando a responsabilidade de migra√ß√£o autom√°tica dele por meio do `spring.jpa.hibernate.ddl-auto=validate`. Este tamb√©m ficou respons√°vel por ser o `ORM` do projeto, por isso precisei identificar ao hibernate o tipo de banco que o projeto estava utilizando, o `PostgreSQL`, e a URL de conex√£o com o banco, al√©m do usu√°rio e senha.

```properties
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
```

O flyway ficou respons√°vel por auxiliar nas migra√ß√µes do banco de dados e guardar os scripts de migra√ß√£o. Para isso, precisei configurar o `application.properties` com as informa√ß√µes do banco de dados, como a URL, usu√°rio e senha. Al√©m disso, tamb√©m configurei o `flyway.schemas` para indicar qual era o schema que o flyway deveria utilizar.

### Dificuldade

### Dificuldade

A principal dificuldade enfrentada no desenvolvimento do projeto foi a cria√ß√£o de um crawler capaz de capturar informa√ß√µes de diferentes sites, cada um com seu pr√≥prio design, estrutura de HTML e padr√µes de publica√ß√£o. Diferentemente de APIs, que geralmente seguem um padr√£o de resposta bem definido, os portais de not√≠cias apresentam grande diversidade na organiza√ß√£o dos elementos, uso de classes CSS, tags e at√© mesmo na forma como o conte√∫do √© carregado (dinamicamente via JavaScript ou de forma est√°tica).

Essa heterogeneidade exige que o crawler seja altamente flex√≠vel e adapt√°vel. Para cada novo portal cadastrado pelo cliente, foi necess√°rio analisar manualmente a estrutura do site, identificar os seletores corretos para t√≠tulos, textos, datas e links, e implementar regras espec√≠ficas de extra√ß√£o. Por√©m, isso n√£o √© escal√°vel, porque se o portal muda seu CSS e HTML, nosso crawler acaba quebrando. 

Al√©m disso, muitos sites implementam mecanismos de prote√ß√£o contra automa√ß√£o, como CAPTCHAs, limita√ß√£o de requisi√ß√µes ou carregamento ass√≠ncrono de conte√∫do, o que aumenta ainda mais a complexidade do desenvolvimento. Para lidar com esses desafios, foi necess√°rio estudar t√©cnicas de parsing de HTML, uso de bibliotecas como o Crawler4J, e implementar estrat√©gias para tornar o crawler mais resiliente a mudan√ßas, como a parametriza√ß√£o dos seletores no banco de dados e a cria√ß√£o de logs detalhados para facilitar o diagn√≥stico de falhas.

Portanto, a dificuldade n√£o esteve apenas na implementa√ß√£o t√©cnica do crawler, mas principalmente na necessidade de constante adapta√ß√£o e manuten√ß√£o frente √† diversidade e √† evolu√ß√£o dos portais de not√≠cias monitorados. Essa experi√™ncia evidenciou a import√¢ncia de projetar solu√ß√µes escal√°veis e facilmente configur√°veis para automa√ß√£o de coleta de dados em ambientes web heterog√™neos. 

## Aprendizados e ganhos com o projeto.

Houve aprofundei mais nos conhecimentos de docker e o devcontainer, que facilitou muito a configura√ß√£o do ambiente de desenvolvimento.

O devcontainer n√£o era requisito para essa API, por√©m, membros do grupo tinham m√°quinas diferentes. Dois membros do grupo tinham windows em m√°quina pr√≥pria, outro membro n√£o tinha computador e usava o da FATEC (que sempre desconfigurava), ou seja, ele precisava baixar e configurar a m√°quina inteira toda vez que precisava usar. Outro membro do grupo tinha uma m√°quina do trabalho, muito fraca, portanto, n√£o podia instalar nada.
O devcontainer foi uma solu√ß√£o para todos esses problemas. Ele √© um container docker que tem todas as depend√™ncias necess√°rias para o projeto, e pode ser aberto em qualquer m√°quina, independente do sistema operacional.

Por√©m, houve muita resist√™ncia de inicio ao uso. At√© que os membros perceberam que era mais complexo configurar uma m√°quina windows do zero, do que abrir o devcontainer.

Durante o desenvolvimento do projeto, o maior aprendizado foi relacionado √† constru√ß√£o do banco de dados utilizando Java Spring Boot, Hibernate e Flyway. Inicialmente, houve conflitos na configura√ß√£o dessas ferramentas, o que exigiu um entendimento detalhado sobre o escopo de cada uma e como elas se comunicam. No arquivo `application.properties`, configurei o Hibernate para atuar como validador das entidades, desativando a migra√ß√£o autom√°tica com a propriedade `spring.jpa.hibernate.ddl-auto=validate`. Al√©m disso, defini o Hibernate como o ORM do projeto, especificando o tipo de banco de dados utilizado (PostgreSQL), a URL de conex√£o, o usu√°rio e a senha. 

O Flyway foi configurado para gerenciar as migra√ß√µes do banco de dados e armazenar os scripts de migra√ß√£o. Para isso, adicionei as informa√ß√µes do banco no `application.properties` e configurei o `flyway.schemas` para indicar o schema a ser utilizado. Essa abordagem garantiu uma separa√ß√£o clara entre as responsabilidades do Hibernate e do Flyway, evitando conflitos.

Outro aprendizado significativo foi o uso de Docker e DevContainer para facilitar a configura√ß√£o do ambiente de desenvolvimento. Essa solu√ß√£o foi essencial para superar desafios enfrentados pela equipe, como a diversidade de sistemas operacionais e limita√ß√µes de hardware. O DevContainer permitiu que todos os membros trabalhassem em um ambiente padronizado, independente do sistema operacional ou das restri√ß√µes das m√°quinas utilizadas.

Por fim, o maior ganho t√©cnico foi o desenvolvimento do crawler, que se tornou o n√∫cleo do projeto. Ele foi projetado para buscar portais e URLs no banco de dados, navegar pelas p√°ginas utilizando a biblioteca Crawler4J, salvar os HTMLs coletados em uma pasta espec√≠fica e processar as not√≠cias com as valida√ß√µes necess√°rias. O campo `noti_text` foi alterado para o tipo `TEXT` no PostgreSQL, permitindo o armazenamento de textos sem limite de caracteres. O crawler foi configurado para ser executado automaticamente ao iniciar o projeto, com a adi√ß√£o de uma valida√ß√£o de gatilho para controlar sua execu√ß√£o.

Essa √© a classe cora√ß√£o do projeto, porque ela dispara o crawler no tempo correto. Ele captura os portais que devem ser crawlerados no dia, seja pela sua frequ√™ncia ser di√°ria, semanal ou mensal. 

```
package com.group.backend.cron;

import org.springframework.scheduling.annotation.Scheduled;

import com.group.backend.crawler.PortaisCrawler;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Component;

@Component
public class CronCrawler {
    PortaisCrawler portaisCrawler;

	static String DAY_TIME = "PT5.0S";
	static String WEEK_TIME = "PT10.0S";
	static String MONTH_TIME = "PT20.0S";

	@Autowired
	public CronCrawler(PortaisCrawler portaisCrawler) {
		this.portaisCrawler = portaisCrawler;
	}

	@Scheduled(cron="0 0 * * * ?")
    @EventListener(ApplicationReadyEvent.class)
	public void dailyCrawlerRegisterSchedule() {
		portaisCrawler.startCrawlForFrequency("diariamente");
	}

	@Scheduled(cron="0 0 0 * * MON")
    @EventListener(ApplicationReadyEvent.class)
	public void weeklyCrawlerRegisterSchedule() {
		portaisCrawler.startCrawlForFrequency("semanalmente");
	}

	@Scheduled(cron = "0 0 1 * * ?")
    @EventListener(ApplicationReadyEvent.class)
	public void monthlyCrawlerRegisterSchedule() {
		portaisCrawler.startCrawlForFrequency("mensalmente");
	}
}
```